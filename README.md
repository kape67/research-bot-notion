# Notion Paper Automation Bot

Automatically search for research papers, generate Korean summaries using Gemini AI, and save them to Notion.

## Features

- ğŸ” **Multi-Source Search**: Search papers from both Semantic Scholar and arXiv
- ğŸ“… **Dynamic Database**: Creates a new database for each search with format `"{query} - {date}"`
- ğŸ‡°ğŸ‡· **Gemini AI Integration**:
  - Batch processing: Process 20 papers in just 1 API call (60x more efficient!)
  - Short summary (1 line) - Shown in table
  - Detailed summary (2-3 paragraphs) - Inside page
  - Architecture description - Inside page
- ğŸ”„ **Smart Fallback**: Automatically switches from Gemini 3 Flash to Gemini 2.5 Pro when quota is exceeded
- ğŸ“Š **Model Tracking**: Records which AI model was used in Notion

## Notion Database Schema

- **Name** (Title): Paper title
- **Keyword** (Multi-select): Categories/keywords
- **Publisher & Year** (Text): Publisher and year
- **Link** (URL): Paper link
- **Summary(í•œê¸€)** (Text): 1-line summary in Korean
- **Generated By** (Text): AI model used

## Installation

### 1. Install Required Packages

```bash
pip install notion-client arxiv google-generativeai requests
```

### 2. Set Environment Variables

Add to your `.bashrc` or `.zshrc`:

```bash
export NOTION_API_KEY="your_notion_api_key"
export NOTION_PAGE_ID="your_parent_page_id"
export GEMINI_API_KEY="your_gemini_api_key"
export SEMANTIC_SCHOLAR_API_KEY="your_semantic_scholar_api_key"  # Optional
```

### 3. Get API Keys

- **Notion API**: https://www.notion.so/my-integrations
- **Gemini API**: https://ai.google.dev/
- **Semantic Scholar API** (optional): https://www.semanticscholar.org/product/api

## Usage

```bash
python3 main.py --query "search term" --limit 20
```

### Options

- `--query`: Topic to search (required)
- `--limit`: Number of papers to fetch (default: 20)

### Examples

```bash
# Search 20 reinforcement learning papers
python3 main.py --query "reinforcement learning"

# Search 10 deep learning papers
python3 main.py --query "deep learning" --limit 10
```

## Performance

- **Batch Processing**: 20 papers â†’ 1 Gemini API call
- **Speed**: About 1-2 minutes (depends on network)
- **Rate Limit Safe**: Can process up to 400 papers per day with 20 API calls limit

## Project Structure

```
paper_automation/
â”œâ”€â”€ main.py                      # Main entry point
â”œâ”€â”€ notion_client_wrapper.py     # Notion API wrapper
â”œâ”€â”€ paper_fetcher.py             # arXiv search
â”œâ”€â”€ semantic_scholar.py          # Semantic Scholar search
â”œâ”€â”€ multi_source_search.py       # Combined search
â”œâ”€â”€ llm_translator.py            # Gemini AI integration
â””â”€â”€ TODO.md                      # Development roadmap
```

## Key Features Explained

### 1. Semantic Scholar API Key

Using an API key guarantees **1 request/second**:

```bash
export SEMANTIC_SCHOLAR_API_KEY="your_key"
```

### 2. Gemini Model Fallback

- **Primary**: `gemini-3-flash` (fast and efficient)
- **Fallback**: `gemini-2.5-pro` (automatically switches when quota exceeded)

### 3. Batch Processing

Generate summaries for 20 papers in one API call to minimize API usage:
- Before: 20 papers Ã— 3 calls = 60 API calls
- Now: 1 API call (60x more efficient!)

## Troubleshooting

### Semantic Scholar 429 Error
Wait 5-10 minutes and retry, or get an API key

### Gemini Quota Exceeded
Automatically switches to Gemini 2.5 Pro

### Notion Error
Check if `NOTION_PAGE_ID` is correct

## License

MIT License

## Contributing

Issues and PRs are always welcome!
