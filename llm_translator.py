import os
import google.generativeai as genai

class GeminiTranslator:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.environ.get("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY not set in environment")
        
        genai.configure(api_key=self.api_key)
        
        # Primary model: Gemini 3 Flash
        self.model_primary = genai.GenerativeModel('gemini-3-flash-preview')
        self.primary_model_name = "gemini-3-flash"
        
        # Fallback model: Gemini 2.5 Pro
        self.model_fallback = genai.GenerativeModel('gemini-2.5-pro')
        self.fallback_model_name = "gemini-2.5-pro"
        
        # Track which model was used
        self.last_used_model = self.primary_model_name
        
    def _generate_with_retry(self, prompt, retries=3):
        import time
        
        # Try primary model first
        for i in range(retries):
            try:
                response = self.model_primary.generate_content(prompt)
                time.sleep(4)  # Rate limit handling
                self.last_used_model = self.primary_model_name
                return response.text.strip()
            except Exception as e:
                error_str = str(e)
                
                # Check if quota exceeded
                if "429" in error_str or "Quota exceeded" in error_str or "quota" in error_str.lower():
                    if i < retries - 1:
                        wait = (i + 1) * 10
                        print(f"    âš ï¸  Gemini 3 Flash quota exceeded. Waiting {wait}s...")
                        time.sleep(wait)
                        continue
                    else:
                        # All retries failed, switch to fallback model
                        print(f"    ðŸ”„ Switching to {self.fallback_model_name}...")
                        break
                else:
                    print(f"Error generating content: {e}")
                    return ""
        
        # Fallback to Gemini 2.0
        try:
            response = self.model_fallback.generate_content(prompt)
            time.sleep(4)
            self.last_used_model = self.fallback_model_name
            print(f"    âœ… Successfully used {self.fallback_model_name}")
            return response.text.strip()
        except Exception as e:
            print(f"Error with fallback model: {e}")
            return ""
    
    def get_last_used_model(self):
        """Return the name of the last used model"""
        return self.last_used_model
    
    def analyze_paper(self, title, abstract, authors):
        """
        Analyze paper and return all summaries in one go (JSON format).
        Returns dict with: short_summary, detailed_summary, architecture
        """
        if not abstract:
            return {
                "short_summary": "ìš”ì•½ ì—†ìŒ",
                "detailed_summary": "",
                "architecture": ""
            }
            
        prompt = f"""ë‹¤ìŒ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë°˜ë“œì‹œ **JSON í˜•ì‹**ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ë…¼ë¬¸ ì œëª©: {title}
ì €ìž: {', '.join(authors[:3])}
ì´ˆë¡: {abstract}

ë‹¤ìŒ í‚¤ë¥¼ ê°€ì§„ JSON ê°ì²´ë¥¼ ìƒì„±í•˜ì„¸ìš”:
1. "short_summary": ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ **í•œ ë¬¸ìž¥**ìœ¼ë¡œ ìš”ì•½ (í•œêµ­ì–´)
2. "detailed_summary": ë…¼ë¬¸ì— ëŒ€í•œìƒì„¸ ìš”ì•½ (2-3ë¬¸ë‹¨, í•œêµ­ì–´). ë¬¸ì œ ì •ì˜, ë°©ë²•ë¡ , ê²°ê³¼ë¥¼ í¬í•¨.
3. "architecture": ì œì•ˆí•˜ëŠ” Architecture ë˜ëŠ” ë°©ë²•ë¡ ì— ëŒ€í•œ ê°„ëžµí•œ ì„¤ëª… (2-3ë¬¸ìž¥, í•œêµ­ì–´)

ì¶œë ¥ ì˜ˆì‹œ:
{{
  "short_summary": "...",
  "detailed_summary": "...",
  "architecture": "..."
}}
"""
        response_text = self._generate_with_retry(prompt)
        
        # Parse JSON
        import json
        import re
        
        try:
            # Remove markdown code blocks if present
            clean_text = re.sub(r'```json\s*|\s*```', '', response_text).strip()
            data = json.loads(clean_text)
            return {
                "short_summary": data.get("short_summary", "ìš”ì•½ ì‹¤íŒ¨"),
                "detailed_summary": data.get("detailed_summary", ""),
                "architecture": data.get("architecture", "")
            }
        except Exception as e:
            print(f"Error parsing JSON: {e}")
            print(f"Raw response: {response_text}")
            return {
                "short_summary": "ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨",
                "detailed_summary": response_text, # Fallback to raw text
                "architecture": ""
            }

    def analyze_papers_batch(self, papers_data):
        """
        Analyze multiple papers in one API call (batch processing).
        papers_data: list of dicts with 'title', 'abstract', 'authors'
        Returns: tuple of (list of dicts with analysis results, model_name)
        """
        if not papers_data:
            return [], "none"
        
        # Build batch prompt
        papers_text = ""
        for idx, paper in enumerate(papers_data):
            authors_str = ', '.join(paper.get('authors', [])[:3])
            papers_text += f"""
---
ë…¼ë¬¸ {idx + 1}:
ì œëª©: {paper['title']}
ì €ìž: {authors_str}
ì´ˆë¡: {paper.get('abstract', 'No abstract available')}
"""
        
        prompt = f"""ë‹¤ìŒ {len(papers_data)}ê°œì˜ ë…¼ë¬¸ì„ ê°ê° ë¶„ì„í•˜ì—¬ JSON ë°°ì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

{papers_text}

ê° ë…¼ë¬¸ì— ëŒ€í•´ ë‹¤ìŒ í‚¤ë¥¼ ê°€ì§„ JSON ê°ì²´ë¥¼ ìƒì„±í•˜ì„¸ìš”:
1. "paper_index": ë…¼ë¬¸ ë²ˆí˜¸ (1ë¶€í„° ì‹œìž‘)
2. "short_summary": ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ **í•œ ë¬¸ìž¥**ìœ¼ë¡œ ìš”ì•½ (í•œêµ­ì–´)
3. "detailed_summary": ë…¼ë¬¸ì— ëŒ€í•œ ìƒì„¸ ìš”ì•½ (2-3ë¬¸ë‹¨, í•œêµ­ì–´). ë¬¸ì œ ì •ì˜, ë°©ë²•ë¡ , ê²°ê³¼ë¥¼ í¬í•¨.
4. "architecture": ì œì•ˆí•˜ëŠ” Architecture ë˜ëŠ” ë°©ë²•ë¡ ì— ëŒ€í•œ ê°„ëžµí•œ ì„¤ëª… (2-3ë¬¸ìž¥, í•œêµ­ì–´)

ì¶œë ¥ í˜•ì‹ (JSON ë°°ì—´):
[
  {{
    "paper_index": 1,
    "short_summary": "...",
    "detailed_summary": "...",
    "architecture": "..."
  }},
  {{
    "paper_index": 2,
    "short_summary": "...",
    "detailed_summary": "...",
    "architecture": "..."
  }}
]
"""
        
        response_text = self._generate_with_retry(prompt)
        used_model = self.last_used_model
        
        # Parse JSON array
        import json
        import re
        
        try:
            # Remove markdown code blocks if present
            clean_text = re.sub(r'```json\s*|\s*```', '', response_text).strip()
            data = json.loads(clean_text)
            
            # Ensure it's a list
            if not isinstance(data, list):
                raise ValueError("Response is not a JSON array")
            
            # Map results back to papers
            results = []
            for paper_data in papers_data:
                results.append({
                    "short_summary": "ìš”ì•½ ì—†ìŒ",
                    "detailed_summary": "",
                    "architecture": ""
                })
            
            # Fill in results from API response
            for item in data:
                idx = item.get("paper_index", 0) - 1
                if 0 <= idx < len(results):
                    results[idx] = {
                        "short_summary": item.get("short_summary", "ìš”ì•½ ì‹¤íŒ¨"),
                        "detailed_summary": item.get("detailed_summary", ""),
                        "architecture": item.get("architecture", "")
                    }
            
            return results, used_model
            
        except Exception as e:
            print(f"Error parsing batch JSON: {e}")
            print(f"Raw response: {response_text[:500]}...")
            # Return empty results for all papers
            return [{
                "short_summary": "ë°°ì¹˜ ìš”ì•½ ì‹¤íŒ¨",
                "detailed_summary": "",
                "architecture": ""
            } for _ in papers_data], used_model

    # Legacy methods removed to enforce single call usage

